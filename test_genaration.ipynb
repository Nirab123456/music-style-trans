{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09fad819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "# Import missing modules for optimization\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchaudio\n",
    "# Import our custom dataset and augmentation pipeline.\n",
    "from process_sml import (\n",
    "    AudioDatasetFolder, Compose,compute_waveform_griffinlim,reconstruct_waveform,\n",
    "    RandomPitchShift_wav,RandomVolume_wav,RandomAbsoluteNoise_wav,RandomSpeed_wav,RandomFade_wav,RandomFrequencyMasking_spec,RandomTimeMasking_spec,RandomTimeStretch_spec)\n",
    "# Import the UNet model and the training function from the training module.\n",
    "from train_sml import UNet, train_model_source_separation,LiteResUNet,infer_and_save,GLRUNET\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the component map for the dataset.\n",
    "COMPONENT_MAP = [\"mixture\", \"drums\", \"bass\", \"other_accompaniment\", \"vocals\"]\n",
    "label_names = [\"drums\", \"bass\", \"other_accompaniment\", \"vocals\"]\n",
    "\n",
    "dataset_val = AudioDatasetFolder(\n",
    "    csv_file='output_stems/test_one.csv',\n",
    "    audio_dir='.',  # adjust as needed\n",
    "    components=COMPONENT_MAP,\n",
    "    sample_rate=16000,\n",
    "    duration=10.0,\n",
    "    is_track_id=True,\n",
    "    input_name= \"mixture\",\n",
    "\n",
    ")\n",
    "data_loader = DataLoader(dataset_val, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3071583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LiteResUNet(backbone=\"resnet18\",source_names=label_names,pretrained=True,in_channels=2)\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e95ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# grab one batch\n",
    "sample_multi = next(iter(data_loader))\n",
    "\n",
    "# Option A) move just the spectrogram you care about\n",
    "spec = sample_multi['vocals'][0]    # shape = (channels, freq, time)\n",
    "spec = spec.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09ff4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 129, 5001])\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# grab one batch\n",
    "sample_multi = next(iter(data_loader))\n",
    "\n",
    "# Option A) move just the spectrogram you care about\n",
    "spec = sample_multi['mixture'][0]    # shape = (channels, freq, time)\n",
    "spec = spec.to(device)\n",
    "print(spec.shape)\n",
    "presaved_weights = torch.load(r\"checkpoints\\checkpoint_epoch_83.pth\")\n",
    "state_of_dict = presaved_weights['model_state_dict']\n",
    "\n",
    "model.load_state_dict(state_dict=state_of_dict)\n",
    "model = model.to(device)\n",
    "# now you can e.g. plot it (if you move it back to CPU first) or feed it into a model:\n",
    "model_input = spec.unsqueeze(0)      # add batch dim if needed\n",
    "model_input = model_input.to(device)\n",
    "out = model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570a5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_multi = next(iter(data_loader))\n",
    "\n",
    "# Option A) move just the spectrogram you care about\n",
    "spec = sample_multi['vocals'][0]    # shape = (channels, freq, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca85737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 129, 5001])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c49c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = spec.size(0) // 2           # 4 // 2 == 2\n",
    "mag   = spec[0:C,  :,  :]       # → [2, 1025, 157]\n",
    "phase = spec[C: ,  :,  :]       # → [2, 1025, 157]\n",
    "complex_spec = torch.polar(mag, phase)\n",
    "reconstruction = reconstruct_waveform(complex_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952ae65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 160000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e31c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 129, 5001])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[\"vocals\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0737e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocals =out[\"vocals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66240461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wav = compute_waveform_griffinlim(mag_spec=spec,n_fft=256,hop_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97179a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav=wav.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03fa3dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.3448)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import peak_signal_noise_ratio\n",
    "\n",
    "peak_signal_noise_ratio(reconstruction.to(device=\"cpu\"), wav.to(device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e292be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 160000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ef92528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[3.3765e-01, 3.3765e-01, 3.0886e-01,  ..., 1.1192e-02,\n",
       "           1.2870e-02, 1.2870e-02],\n",
       "          [3.3765e-01, 3.3765e-01, 3.0886e-01,  ..., 1.1192e-02,\n",
       "           1.2870e-02, 1.2870e-02],\n",
       "          [2.9694e-01, 2.9694e-01, 2.7252e-01,  ..., 9.3838e-03,\n",
       "           1.0822e-02, 1.0822e-02],\n",
       "          ...,\n",
       "          [1.0166e+01, 1.0166e+01, 9.1265e+00,  ..., 9.5611e-03,\n",
       "           1.0430e-02, 1.0430e-02],\n",
       "          [8.8001e+00, 8.8001e+00, 7.9124e+00,  ..., 1.1152e-02,\n",
       "           1.2243e-02, 1.2243e-02],\n",
       "          [8.8001e+00, 8.8001e+00, 7.9124e+00,  ..., 1.1152e-02,\n",
       "           1.2243e-02, 1.2243e-02]],\n",
       "\n",
       "         [[3.3652e-01, 3.3652e-01, 3.0712e-01,  ..., 2.0638e-02,\n",
       "           2.3692e-02, 2.3692e-02],\n",
       "          [3.3652e-01, 3.3652e-01, 3.0712e-01,  ..., 2.0638e-02,\n",
       "           2.3692e-02, 2.3692e-02],\n",
       "          [2.9528e-01, 2.9528e-01, 2.7038e-01,  ..., 1.8374e-02,\n",
       "           2.0996e-02, 2.0996e-02],\n",
       "          ...,\n",
       "          [1.0129e+01, 1.0129e+01, 9.0952e+00,  ..., 7.5931e-03,\n",
       "           8.2966e-03, 8.2966e-03],\n",
       "          [8.7549e+00, 8.7549e+00, 7.8751e+00,  ..., 9.1709e-03,\n",
       "           1.0105e-02, 1.0105e-02],\n",
       "          [8.7549e+00, 8.7549e+00, 7.8751e+00,  ..., 9.1709e-03,\n",
       "           1.0105e-02, 1.0105e-02]]]], device='cuda:0',\n",
       "       grad_fn=<UpsampleBilinear2DBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocals.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fccc47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inverse_melscale_transform = torchaudio.transforms.InverseMelScale(n_stft=2048 // 2 + 1)\n",
    "inverse_melscale_transform.to(device)\n",
    "spectrogram = inverse_melscale_transform(vocals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43adf553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1025, 313])\n",
      "shape of the meg torch.Size([1, 2, 1025, 313])\n",
      "shape of the flat torch.Size([2, 1025, 313])\n",
      "shape of the meg torch.Size([1, 2, 1025, 313])\n",
      "shape of the flat torch.Size([2, 1025, 313])\n",
      "shape of the meg torch.Size([1, 2, 1025, 313])\n",
      "shape of the flat torch.Size([2, 1025, 313])\n",
      "shape of the meg torch.Size([1, 2, 1025, 313])\n",
      "shape of the flat torch.Size([2, 1025, 313])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 319488]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     36\u001b[39m waveforms = {}\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, spec \u001b[38;5;129;01min\u001b[39;00m preds.items():\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# spec: (B, C_out, F, T)  —  ideally this is power spectrogram or magnitude\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# if your GL layer is stored in `model.griffinlim` you can call:\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     wave = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# (B, C_out, L) waveform tensor\u001b[39;00m\n\u001b[32m     41\u001b[39m     waveforms[name] = wave.cpu()          \u001b[38;5;66;03m# or keep on device as you wish\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\music_projects\\music-style-trans\\train_sml\\model_griffinlim.py:113\u001b[39m, in \u001b[36mGLRUNET.forward\u001b[39m\u001b[34m(self, x, apply_gl)\u001b[39m\n\u001b[32m    110\u001b[39m do_gl = \u001b[38;5;28mself\u001b[39m.use_gl \u001b[38;5;28;01mif\u001b[39;00m apply_gl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m apply_gl\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# -- encoder pass --\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m e1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m e2 = \u001b[38;5;28mself\u001b[39m.enc2(e1)\n\u001b[32m    115\u001b[39m e3 = \u001b[38;5;28mself\u001b[39m.enc3(e2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rifat\\miniconda3\\envs\\smal\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 319488]"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# grab one batch\n",
    "sample_multi = next(iter(data_loader))\n",
    "\n",
    "# Option A) move just the spectrogram you care about\n",
    "spec = sample_multi['mixture'][0]    # shape = (channels, freq, time)\n",
    "\n",
    "spec = spec.unsqueeze(0)\n",
    "spec = spec.to(device)\n",
    "\n",
    "model = GLRUNET(backbone=\"resnet18\",source_names=label_names,pretrained=True,in_channels=2, use_griffinlim=True)  \n",
    "model.to(device)\n",
    "\n",
    "# 2) Load your checkpoint, but allow GL’s own parameters to stay at their defaults\n",
    "presaved_weights = torch.load(r\"ALL_CKP\\semi_success_ckp\\spec-griffinlim.pth\")\n",
    "state_of_dict = presaved_weights['model_state_dict']\n",
    "model.load_state_dict(state_of_dict, strict=False)\n",
    "\n",
    "# 3) Switch the model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# 4) At inference time, you want two steps:\n",
    "#    (a) run the UNet backbone + decoder to get your output spectrograms\n",
    "#    (b) apply Griffin–Lim (either via your built‑in layer or directly) to get waveforms\n",
    "\n",
    "# suppose `mixture_spec` is your input magnitude (B, C_in, F, T) torch‑Tensor on `device`\n",
    "with torch.no_grad():\n",
    "    # 4a) get your raw spectrogram predictions\n",
    "    print(spec.shape)\n",
    "    preds = model(spec)    # Dict[str, Tensor], each Tensor = (B, C_out, F, T)\n",
    "\n",
    "    # if you coded your final convs to output just magnitudes,\n",
    "    # you can call your GL layer like this:\n",
    "    model.use_griffinlim = True    # turn it on\n",
    "    waveforms = {}\n",
    "    for name, spec in preds.items():\n",
    "        # spec: (B, C_out, F, T)  —  ideally this is power spectrogram or magnitude\n",
    "        # if your GL layer is stored in `model.griffinlim` you can call:\n",
    "        wave = model(spec)         # (B, C_out, L) waveform tensor\n",
    "        waveforms[name] = wave.cpu()          # or keep on device as you wish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45eeabd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 64, 64, 157]           6,272\n",
      "       BatchNorm2d-2           [1, 64, 64, 157]             128\n",
      "              ReLU-3           [1, 64, 64, 157]               0\n",
      "         MaxPool2d-4            [1, 64, 32, 79]               0\n",
      "            Conv2d-5            [1, 64, 32, 79]          36,864\n",
      "       BatchNorm2d-6            [1, 64, 32, 79]             128\n",
      "              ReLU-7            [1, 64, 32, 79]               0\n",
      "            Conv2d-8            [1, 64, 32, 79]          36,864\n",
      "       BatchNorm2d-9            [1, 64, 32, 79]             128\n",
      "             ReLU-10            [1, 64, 32, 79]               0\n",
      "       BasicBlock-11            [1, 64, 32, 79]               0\n",
      "           Conv2d-12            [1, 64, 32, 79]          36,864\n",
      "      BatchNorm2d-13            [1, 64, 32, 79]             128\n",
      "             ReLU-14            [1, 64, 32, 79]               0\n",
      "           Conv2d-15            [1, 64, 32, 79]          36,864\n",
      "      BatchNorm2d-16            [1, 64, 32, 79]             128\n",
      "             ReLU-17            [1, 64, 32, 79]               0\n",
      "       BasicBlock-18            [1, 64, 32, 79]               0\n",
      "           Conv2d-19           [1, 128, 16, 40]          73,728\n",
      "      BatchNorm2d-20           [1, 128, 16, 40]             256\n",
      "             ReLU-21           [1, 128, 16, 40]               0\n",
      "           Conv2d-22           [1, 128, 16, 40]         147,456\n",
      "      BatchNorm2d-23           [1, 128, 16, 40]             256\n",
      "           Conv2d-24           [1, 128, 16, 40]           8,192\n",
      "      BatchNorm2d-25           [1, 128, 16, 40]             256\n",
      "             ReLU-26           [1, 128, 16, 40]               0\n",
      "       BasicBlock-27           [1, 128, 16, 40]               0\n",
      "           Conv2d-28           [1, 128, 16, 40]         147,456\n",
      "      BatchNorm2d-29           [1, 128, 16, 40]             256\n",
      "             ReLU-30           [1, 128, 16, 40]               0\n",
      "           Conv2d-31           [1, 128, 16, 40]         147,456\n",
      "      BatchNorm2d-32           [1, 128, 16, 40]             256\n",
      "             ReLU-33           [1, 128, 16, 40]               0\n",
      "       BasicBlock-34           [1, 128, 16, 40]               0\n",
      "           Conv2d-35            [1, 256, 8, 20]         294,912\n",
      "      BatchNorm2d-36            [1, 256, 8, 20]             512\n",
      "             ReLU-37            [1, 256, 8, 20]               0\n",
      "           Conv2d-38            [1, 256, 8, 20]         589,824\n",
      "      BatchNorm2d-39            [1, 256, 8, 20]             512\n",
      "           Conv2d-40            [1, 256, 8, 20]          32,768\n",
      "      BatchNorm2d-41            [1, 256, 8, 20]             512\n",
      "             ReLU-42            [1, 256, 8, 20]               0\n",
      "       BasicBlock-43            [1, 256, 8, 20]               0\n",
      "           Conv2d-44            [1, 256, 8, 20]         589,824\n",
      "      BatchNorm2d-45            [1, 256, 8, 20]             512\n",
      "             ReLU-46            [1, 256, 8, 20]               0\n",
      "           Conv2d-47            [1, 256, 8, 20]         589,824\n",
      "      BatchNorm2d-48            [1, 256, 8, 20]             512\n",
      "             ReLU-49            [1, 256, 8, 20]               0\n",
      "       BasicBlock-50            [1, 256, 8, 20]               0\n",
      "           Conv2d-51            [1, 512, 4, 10]       1,179,648\n",
      "      BatchNorm2d-52            [1, 512, 4, 10]           1,024\n",
      "             ReLU-53            [1, 512, 4, 10]               0\n",
      "           Conv2d-54            [1, 512, 4, 10]       2,359,296\n",
      "      BatchNorm2d-55            [1, 512, 4, 10]           1,024\n",
      "           Conv2d-56            [1, 512, 4, 10]         131,072\n",
      "      BatchNorm2d-57            [1, 512, 4, 10]           1,024\n",
      "             ReLU-58            [1, 512, 4, 10]               0\n",
      "       BasicBlock-59            [1, 512, 4, 10]               0\n",
      "           Conv2d-60            [1, 512, 4, 10]       2,359,296\n",
      "      BatchNorm2d-61            [1, 512, 4, 10]           1,024\n",
      "             ReLU-62            [1, 512, 4, 10]               0\n",
      "           Conv2d-63            [1, 512, 4, 10]       2,359,296\n",
      "      BatchNorm2d-64            [1, 512, 4, 10]           1,024\n",
      "             ReLU-65            [1, 512, 4, 10]               0\n",
      "       BasicBlock-66            [1, 512, 4, 10]               0\n",
      "  ConvTranspose2d-67            [1, 256, 8, 20]         524,544\n",
      "           Conv2d-68            [1, 256, 8, 20]       1,179,904\n",
      "      BatchNorm2d-69            [1, 256, 8, 20]             512\n",
      "             ReLU-70            [1, 256, 8, 20]               0\n",
      "           Conv2d-71            [1, 256, 8, 20]         590,080\n",
      "      BatchNorm2d-72            [1, 256, 8, 20]             512\n",
      "             ReLU-73            [1, 256, 8, 20]               0\n",
      "  ConvTranspose2d-74           [1, 128, 16, 40]         131,200\n",
      "           Conv2d-75           [1, 128, 16, 40]         295,040\n",
      "      BatchNorm2d-76           [1, 128, 16, 40]             256\n",
      "             ReLU-77           [1, 128, 16, 40]               0\n",
      "           Conv2d-78           [1, 128, 16, 40]         147,584\n",
      "      BatchNorm2d-79           [1, 128, 16, 40]             256\n",
      "             ReLU-80           [1, 128, 16, 40]               0\n",
      "  ConvTranspose2d-81            [1, 64, 32, 80]          32,832\n",
      "           Conv2d-82            [1, 64, 32, 79]          73,792\n",
      "      BatchNorm2d-83            [1, 64, 32, 79]             128\n",
      "             ReLU-84            [1, 64, 32, 79]               0\n",
      "           Conv2d-85            [1, 64, 32, 79]          36,928\n",
      "      BatchNorm2d-86            [1, 64, 32, 79]             128\n",
      "             ReLU-87            [1, 64, 32, 79]               0\n",
      "  ConvTranspose2d-88           [1, 64, 64, 158]          16,448\n",
      "           Conv2d-89            [1, 64, 32, 79]          73,792\n",
      "      BatchNorm2d-90            [1, 64, 32, 79]             128\n",
      "             ReLU-91            [1, 64, 32, 79]               0\n",
      "           Conv2d-92            [1, 64, 32, 79]          36,928\n",
      "      BatchNorm2d-93            [1, 64, 32, 79]             128\n",
      "             ReLU-94            [1, 64, 32, 79]               0\n",
      "           Conv2d-95             [1, 2, 32, 79]             130\n",
      "           Conv2d-96             [1, 2, 32, 79]             130\n",
      "           Conv2d-97             [1, 2, 32, 79]             130\n",
      "           Conv2d-98             [1, 2, 32, 79]             130\n",
      "================================================================\n",
      "Total params: 14,315,016\n",
      "Trainable params: 14,315,016\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.31\n",
      "Forward/backward pass size (MB): 78.45\n",
      "Params size (MB): 54.61\n",
      "Estimated Total Size (MB): 133.36\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model.to(device)\n",
    "summary(\n",
    "    model, \n",
    "    input_size=(2, 128, 313),    # omit batch dim\n",
    "    batch_size=1,                # optional, for FLOPs estimate\n",
    "    device=str(device)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926f7680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak RAM: 67.6708984375 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.eval()\n",
    "dummy = torch.randn(1, 2, 128, 313, device=device)\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "with torch.no_grad():\n",
    "    _ = model(dummy)\n",
    "print(\"Peak RAM:\", torch.cuda.max_memory_allocated() / (1024**2), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef6c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8198fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc7e6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab one batch\n",
    "sample_multi = next(iter(data_loader))\n",
    "\n",
    "# Option A) move just the spectrogram you care about\n",
    "spec = sample_multi['mixture'][0]    # shape = (channels, freq, time)\n",
    "spec = spec.to(device)\n",
    "model = model.to(device)\n",
    "# now you can e.g. plot it (if you move it back to CPU first) or feed it into a model:\n",
    "model_input = spec.unsqueeze(0)      # add batch dim if needed\n",
    "model_input = model_input.to(device)\n",
    "out = model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a10f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocals = out[\"vocals\"].squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30498f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_melscale_transform = torchaudio.transforms.InverseMelScale(n_stft=2048 // 2 + 1)\n",
    "inverse_melscale_transform.to(device)\n",
    "spectrogram = inverse_melscale_transform(vocals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24961c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1025, 313])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4aab3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1025, 313])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5819737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_spec= compute_waveform_griffinlim(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cd71786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 159744])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb593b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 159744])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34fe201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "# wav1 is shape [2, 64000], dtype=float\n",
    "waveform = wav.squeeze().detach().cpu()     # now shape [2, 64000]\n",
    "torchaudio.save(\"vocal_D-recon-f-256-h-32.wav\", waveform, sample_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f80d203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All inference outputs saved to ./inference_outputs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# after training:\n",
    "infer_and_save(\n",
    "    model=model,\n",
    "    dataloader=data_loader,\n",
    "    device=device,\n",
    "    output_dir=\"./inference_outputs\",\n",
    "    input_name=\"mixture\",\n",
    "    label_names=[\"drums\", \"bass\", \"other_accompaniment\", \"vocals\"],\n",
    "    sample_rate=16000,\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
