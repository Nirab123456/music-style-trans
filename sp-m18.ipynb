{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f744002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "# Import missing modules for optimization\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Import the UNet model and the training function from the training module.\n",
    "\n",
    "from train_sml import UNet, train_model_source_separation\n",
    "import torch.nn as nn\n",
    "# Import our custom dataset and augmentation pipeline.\n",
    "from process_sml import AudioDatasetFolder, Compose, RandomTimeCrop, RandomTimeStretch, RandomPitchShift, RandomNoise, RandomDistortion, RandomVolume,compute_waveform,to_stereo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a198bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_pipeline = Compose([\n",
    "    RandomTimeCrop(target_time=512),\n",
    "    # RandomTimeStretch(factor_range=(0.9, 1.1)),\n",
    "    RandomPitchShift(shift_range=(-1.0, 1.0)),\n",
    "    RandomNoise(noise_std=0.05),\n",
    "    RandomDistortion(gamma_range=(0.8, 1.2)),\n",
    "    RandomVolume(volume_range=(0.8, 1.2))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa21788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the component map for the dataset.\n",
    "COMPONENT_MAP = [\"mixture\", \"drums\", \"bass\", \"other_accompaniment\", \"vocals\"]\n",
    "IS_TRACK_ID = True\n",
    "\n",
    "# Set random seeds for reproducibility.\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Choose device early.\n",
    "device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the dataset.\n",
    "dataset_multi = AudioDatasetFolder(\n",
    "    csv_file='output_stems/musdb18_index_20250408_121813.csv',\n",
    "    audio_dir='.',  # adjust as needed\n",
    "    components=COMPONENT_MAP,\n",
    "    sample_rate=16000,\n",
    "    duration=5.0,\n",
    "    # transform=augmentation_pipeline,  # list of transforms\n",
    "    is_track_id=IS_TRACK_ID,\n",
    "    input_name= \"mixture\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4141b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1025, 157])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dataset_multi.__getitem__(3)['mixture']\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b6214ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "\n",
    "# Make sure wav is 1D (mono) or 2D (2, N) (stereo)\n",
    "def prepare_for_playback(wav: torch.Tensor) -> torch.Tensor:\n",
    "    if wav.dim() == 2:\n",
    "        # shape: (channels, time)\n",
    "        if wav.size(0) > 2:\n",
    "            wav = wav[:2]  # take first two channels only\n",
    "        return wav\n",
    "    elif wav.dim() == 1:\n",
    "        return wav\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected waveform shape\")\n",
    "\n",
    "# Example\n",
    "x = dataset_multi.__getitem__(3)\n",
    "wav = compute_waveform(x['mixture'])\n",
    "\n",
    "# Fix shape for playback\n",
    "wav = prepare_for_playback(wav)\n",
    "\n",
    "# Convert to numpy and transpose if stereo\n",
    "wav_np = wav.cpu().numpy()\n",
    "if wav_np.ndim == 2:\n",
    "    wav_np = wav_np.T  # (channels, time) â†’ (time, channels)\n",
    "\n",
    "# Play\n",
    "sd.play(wav_np, samplerate=16000)\n",
    "sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "004e0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split dataset into train and validation (e.g., 80/20 split).\n",
    "dataset_size = len(dataset_multi)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(0.8 * dataset_size)\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "train_loader = DataLoader(dataset_multi, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset_multi, batch_size=32, sampler=val_sampler)\n",
    "dataloaders: Dict[str, DataLoader] = {\"train\": train_loader, \"val\": val_loader}\n",
    "\n",
    "# -------------------------------\n",
    "# Model Integration\n",
    "# -------------------------------\n",
    "# For source separation, the model is expected to take the mixture spectrogram as input,\n",
    "# and output separated source spectrograms corresponding to each target.\n",
    "# --- Since the mixture is stereo, we initialize the UNet with in_channels=2 ---\n",
    "model = UNet(in_channels=2)\n",
    "\n",
    "# Define the label names (target keys) for source separation.\n",
    "label_names = [\"drums\", \"bass\", \"other_accompaniment\", \"vocals\"]\n",
    "\n",
    "# Prepare the final convolution layers for each target output.\n",
    "# Here we assume that the decoder produces feature maps with 16 channels.\n",
    "for key in label_names:\n",
    "    model.final_convs[key] = nn.Conv2d(16, 2, kernel_size=1)\n",
    "\n",
    "# IMPORTANT: Move the entire model to the device after adding the final conv layers.\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2dc1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "# Your input\n",
    "input_shape = (2, 1025, 32)\n",
    "\n",
    "\n",
    "# Summary with all output channels\n",
    "summary(model=model, input_size=input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0b761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------------------------------------\n",
      "Train Epoch [1/3] Batch [0/4] Loss: 0.9941 LR: 0.001000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Loss Function, Optimizer, Scheduler\n",
    "# -------------------------------\n",
    "# Use L1 loss for source separation.\n",
    "criterion = nn.L1Loss()\n",
    "# Create the optimizer using the model parameters.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# Create a learning rate scheduler.\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# -------------------------------\n",
    "# Train the Model\n",
    "# -------------------------------\n",
    "# Here, the input key is \"mixture\" and label names are defined as above.\n",
    "best_model = train_model_source_separation(\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=3,\n",
    "    device=device,\n",
    "    log_dir='./logs',\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    input_name=\"mixture\",  # use \"mixture\" for the input spectrogram from the batch\n",
    "    label_names=label_names,  # list of target keys for separated sources\n",
    "    print_freq=10,\n",
    ")\n",
    "\n",
    "# (Optional) Test or visualize the best_model as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875220f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define an additional simple normalization transform:\n",
    "def normalize_spec(spec: torch.Tensor) -> torch.Tensor:\n",
    "    return (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "\n",
    "COMPONENT_MAP = [\"mixture\", \"drums\", \"bass\", \"other_accompaniment\", \"vocals\"]\n",
    "\n",
    "IS_TRACK_ID = True\n",
    "dataset_multi = AudioDatasetFolder(\n",
    "    csv_file='output_stems/musdb18_index_20250408_121813.csv',\n",
    "    audio_dir='.',  # adjust as needed\n",
    "    components=COMPONENT_MAP,\n",
    "    sample_rate=16000,\n",
    "    duration=5.0,\n",
    "    is_track_id=IS_TRACK_ID,\n",
    ")\n",
    "\n",
    "loader_multi = DataLoader(dataset_multi, batch_size=32, shuffle=False)\n",
    "sample_multi = next(iter(loader_multi))\n",
    "\n",
    "if IS_TRACK_ID:\n",
    "    print(\"Loaded sample with track_id:\", sample_multi['track_id'])\n",
    "\n",
    "# Plot spectrogram for the 'mixture' component.\n",
    "spec_multi = sample_multi['vocals'][0, 0]  # select first sample and first channel\n",
    "plt.imshow(spec_multi.detach().numpy(), origin='lower', aspect='auto', cmap='Dark2_r')\n",
    "plt.title(\"Spectrogram (mixture) with pitch_shift and normalization\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency Bin\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random crop\n",
    "def random_noise_crop(tensor, crop_duration=5.0, sample_rate=16000):\n",
    "    crop_size = int(crop_duration * sample_rate)\n",
    "    max_start = tensor.shape[1] - crop_size\n",
    "    start = random.randint(0, max_start)\n",
    "    return tensor[:, start:start + crop_size]\n",
    "\n",
    "noise_crop = random_noise_crop(big_tensor)\n",
    "# Convert to spectrogram\n",
    "spec = compute_spectrogram(noise_crop)\n",
    "print(f\"Shape of 5-second noise spectrogram: {spec.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
