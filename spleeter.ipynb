{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "\"\"\"\n",
    "PyTorch/Torchaudio Audio Processing Library and Training Script\n",
    "\n",
    "This file provides:\n",
    "  - An abstract AudioAdapter and a concrete TorchaudioAdapter implementation.\n",
    "  - Converter functions for channel manipulation and gain/dB conversions.\n",
    "  - Spectrogram computation and simple augmentation functions (time stretch, pitch shift).\n",
    "  - An AudioDataset class that reads audio file paths from a CSV file, loads the waveform,\n",
    "    computes spectrograms, and yields batches for training.\n",
    "  - A simple convolutional network (a lightweight U-Net style model) for source separation.\n",
    "  - A training loop that demonstrates training on the MUSDB18 dataset.\n",
    "\n",
    "Author: Your Name\n",
    "License: MIT License\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "# =============================================================================\n",
    "# AudioAdapter and Concrete TorchaudioAdapter\n",
    "# =============================================================================\n",
    "\n",
    "class AudioAdapter(ABC):\n",
    "    \"\"\"\n",
    "    An abstract class for manipulating audio signals.\n",
    "    \"\"\"\n",
    "\n",
    "    _DEFAULT: Optional[\"AudioAdapter\"] = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(\n",
    "        self,\n",
    "        audio_path: Union[str, Path],\n",
    "        offset: Optional[float] = None,\n",
    "        duration: Optional[float] = None,\n",
    "        sample_rate: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Loads an audio file and returns a waveform tensor and sample rate.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(\n",
    "        self,\n",
    "        path: Union[str, Path],\n",
    "        data: torch.Tensor,\n",
    "        sample_rate: int,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Saves a waveform tensor to an audio file.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def default(cls) -> \"AudioAdapter\":\n",
    "        if cls._DEFAULT is None:\n",
    "            cls._DEFAULT = TorchaudioAdapter()\n",
    "        return cls._DEFAULT\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, descriptor: str) -> \"AudioAdapter\":\n",
    "        # For simplicity, return the default adapter.\n",
    "        return cls.default()\n",
    "\n",
    "\n",
    "class TorchaudioAdapter(AudioAdapter):\n",
    "    \"\"\"\n",
    "    A concrete AudioAdapter implementation using torchaudio.\n",
    "    \"\"\"\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        audio_path: Union[str, Path],\n",
    "        offset: Optional[float] = None,\n",
    "        duration: Optional[float] = None,\n",
    "        sample_rate: Optional[int] = None,\n",
    "    ) -> Tuple[torch.Tensor, int]:\n",
    "        # Convert Path to string if necessary.\n",
    "        if isinstance(audio_path, Path):\n",
    "            audio_path = str(audio_path)\n",
    "        # torchaudio.load returns a waveform of shape (channels, samples)\n",
    "        waveform, orig_sr = torchaudio.load(audio_path, normalize=True)\n",
    "        # Apply offset and duration if specified.\n",
    "        if offset is not None or duration is not None:\n",
    "            start = int(offset * orig_sr) if offset is not None else 0\n",
    "            if duration is not None:\n",
    "                end = start + int(duration * orig_sr)\n",
    "            else:\n",
    "                end = waveform.size(1)\n",
    "            waveform = waveform[:, start:end]\n",
    "        # Resample if a different sample rate is requested.\n",
    "        if sample_rate is not None and sample_rate != orig_sr:\n",
    "            resampler = T.Resample(orig_sr, sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "            orig_sr = sample_rate\n",
    "        return waveform, orig_sr\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        path: Union[str, Path],\n",
    "        data: torch.Tensor,\n",
    "        sample_rate: int,\n",
    "    ) -> None:\n",
    "        if isinstance(path, Path):\n",
    "            path = str(path)\n",
    "        # torchaudio.save expects tensor shape (channels, samples)\n",
    "        torchaudio.save(path, data, sample_rate)\n",
    "\n",
    "# =============================================================================\n",
    "# Converter Functions\n",
    "# =============================================================================\n",
    "\n",
    "def to_n_channels(waveform: torch.Tensor, n_channels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Ensure that the waveform has exactly n_channels.\n",
    "    If there are fewer channels, repeat them; if more, slice.\n",
    "    \"\"\"\n",
    "    current = waveform.size(0)\n",
    "    if current >= n_channels:\n",
    "        return waveform[:n_channels]\n",
    "    else:\n",
    "        return waveform.expand(n_channels, -1)[:n_channels]\n",
    "\n",
    "def to_stereo(waveform: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a waveform to stereo.\n",
    "    \"\"\"\n",
    "    if waveform.size(0) == 1:\n",
    "        return waveform.repeat(2, 1)\n",
    "    elif waveform.size(0) > 2:\n",
    "        return waveform[:2]\n",
    "    return waveform\n",
    "\n",
    "def gain_to_db(tensor: torch.Tensor, epsilon: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert linear gain to decibels.\n",
    "    \"\"\"\n",
    "    return 20.0 * torch.log10(torch.clamp(tensor, min=epsilon))\n",
    "\n",
    "def db_to_gain(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert decibels to linear gain.\n",
    "    \"\"\"\n",
    "    return torch.pow(10.0, tensor / 20.0)\n",
    "\n",
    "# =============================================================================\n",
    "# Spectrogram and Data Augmentation Functions\n",
    "# =============================================================================\n",
    "\n",
    "def compute_spectrogram(\n",
    "    waveform: torch.Tensor,\n",
    "    n_fft: int = 2048,\n",
    "    hop_length: int = 512,\n",
    "    power: float = 1.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the spectrogram (magnitude) of a waveform.\n",
    "    Returns a tensor of shape (channels, freq_bins, time_frames).\n",
    "    \"\"\"\n",
    "    spec_complex = torch.stft(\n",
    "        waveform,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        return_complex=True,\n",
    "        center=True,\n",
    "    )\n",
    "    magnitude = spec_complex.abs() ** power\n",
    "    return magnitude\n",
    "\n",
    "def time_stretch(spectrogram: torch.Tensor, factor: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Time-stretch a spectrogram by a given factor using interpolation.\n",
    "    Assumes spectrogram shape is (channels, freq, time).\n",
    "    \"\"\"\n",
    "    channels, freq, time = spectrogram.shape\n",
    "    new_time = int(time * factor)\n",
    "    # Interpolate along the time dimension (unsqueeze to add batch dimension).\n",
    "    stretched = F.interpolate(spectrogram.unsqueeze(0), size=(freq, new_time),\n",
    "                              mode='bilinear', align_corners=False).squeeze(0)\n",
    "    # Crop or pad to original time length.\n",
    "    if stretched.shape[-1] > time:\n",
    "        stretched = stretched[..., :time]\n",
    "    elif stretched.shape[-1] < time:\n",
    "        pad_amount = time - stretched.shape[-1]\n",
    "        stretched = F.pad(stretched, (0, pad_amount))\n",
    "    return stretched\n",
    "\n",
    "def pitch_shift(spectrogram: torch.Tensor, semitone_shift: float = 0.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Shift the pitch of a spectrogram by a given number of semitones.\n",
    "    This function rescales the frequency axis.\n",
    "    \"\"\"\n",
    "    factor = 2 ** (semitone_shift / 12.0)\n",
    "    channels, freq, time = spectrogram.shape\n",
    "    new_freq = int(freq * factor)\n",
    "    shifted = F.interpolate(spectrogram.unsqueeze(0), size=(new_freq, time),\n",
    "                            mode='bilinear', align_corners=False).squeeze(0)\n",
    "    # Crop or pad frequency dimension back to original.\n",
    "    if new_freq > freq:\n",
    "        shifted = shifted[:, :freq, :]\n",
    "    elif new_freq < freq:\n",
    "        pad_amount = freq - new_freq\n",
    "        shifted = F.pad(shifted, (0, 0, 0, pad_amount))\n",
    "    return shifted\n",
    "\n",
    "# =============================================================================\n",
    "# Dataset Module\n",
    "# =============================================================================\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset that reads audio file paths and metadata from a CSV file,\n",
    "    loads the waveform using an AudioAdapter, computes the spectrogram, and applies\n",
    "    optional augmentation transforms.\n",
    "    \n",
    "    CSV file format:\n",
    "      - Must contain at least a column named 'file' with relative paths to audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file: str,\n",
    "        audio_path: str,\n",
    "        audio_adapter: AudioAdapter,\n",
    "        sample_rate: int = 44100,\n",
    "        duration: float = 20.0,\n",
    "        transform: Optional[Any] = None,\n",
    "    ):\n",
    "        self.samples: List[Dict[str, str]] = []\n",
    "        self.audio_path = audio_path\n",
    "        self.audio_adapter = audio_adapter\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(csv_file, 'r', newline='') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.samples.append(row)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        sample = self.samples[idx]\n",
    "        # Build the full path to the audio file.\n",
    "        file_path = Path(self.audio_path) / sample['file']\n",
    "        # Load the waveform using the provided audio adapter.\n",
    "        waveform, sr = self.audio_adapter.load(file_path, duration=self.duration, sample_rate=self.sample_rate)\n",
    "        # Optionally, convert to stereo.\n",
    "        waveform = to_stereo(waveform)\n",
    "        # Compute the spectrogram.\n",
    "        spec = compute_spectrogram(waveform, n_fft=2048, hop_length=512, power=1.0)\n",
    "        # Apply any additional transforms (e.g., time stretch, pitch shift).\n",
    "        if self.transform:\n",
    "            spec = self.transform(spec)\n",
    "        return spec\n",
    "\n",
    "# =============================================================================\n",
    "# Define a Lightweight Model (Simple U-Net style architecture)\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleSeparationModel(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=16):\n",
    "        super(SimpleSeparationModel, self).__init__()\n",
    "        # Encoder: reduce time-frequency resolution.\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        # Decoder: reconstruct the spectrogram.\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(features, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# =============================================================================\n",
    "# Training Setup\n",
    "# =============================================================================\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in dataloader:\n",
    "            # Expecting batch shape: (batch_size, channels, freq_bins, time_frames)\n",
    "            inputs = batch.to(device)\n",
    "            targets = batch.to(device)  # In practice, replace with true isolated sources.\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(dataloader):.6f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Main Function: Data Loading and Training Loop\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    # Set device (CPU or CUDA)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Set up paths and parameters\n",
    "    csv_file = 'musdb18_train.csv'      # CSV with a column 'file'\n",
    "    audio_folder = 'musdb18_audio'        # Folder containing the MUSDB18 audio files\n",
    "    sample_rate = 44100\n",
    "    duration = 20.0                       # seconds per sample\n",
    "\n",
    "    # Create the audio adapter.\n",
    "    adapter = AudioAdapter.default()\n",
    "\n",
    "    # Optional augmentation transform.\n",
    "    def augment(spec: torch.Tensor) -> torch.Tensor:\n",
    "        # For demonstration, use fixed factors (can randomize for real augmentation).\n",
    "        spec = time_stretch(spec, factor=1.0)\n",
    "        spec = pitch_shift(spec, semitone_shift=0.0)\n",
    "        return spec\n",
    "\n",
    "    # Create the dataset.\n",
    "    dataset = AudioDataset(\n",
    "        csv_file=csv_file,\n",
    "        audio_path=audio_folder,\n",
    "        audio_adapter=adapter,\n",
    "        sample_rate=sample_rate,\n",
    "        duration=duration,\n",
    "        transform=augment,  # Set to None if no augmentation is desired.\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader for batching.\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Instantiate the model (assumes single-channel spectrogram input).\n",
    "    model = SimpleSeparationModel(in_channels=1, out_channels=1, features=16).to(device)\n",
    "\n",
    "    # Define loss function and optimizer.\n",
    "    criterion = nn.L1Loss()  # L1 loss is common for source separation.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Train the model.\n",
    "    train(model, dataloader, criterion, optimizer, device, num_epochs=10)\n",
    "\n",
    "    # Save the model after training.\n",
    "    torch.save(model.state_dict(), \"lightweight_separation_model.pth\")\n",
    "    print(\"Training complete and model saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "spleeter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
